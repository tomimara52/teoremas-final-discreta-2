% Created 2024-06-26 Wed 18:37
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[spanish]{babel}
\setcounter{secnumdepth}{1}
\author{Tomás Maraschio}
\date{\today}
\title{Teoremas para el final de matemática discreta II 2024}
\hypersetup{
 pdfauthor={Tomás Maraschio},
 pdftitle={Teoremas para el final de matemática discreta II 2024},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.6.15)}, 
 pdflang={Spanish}}
\begin{document}

\maketitle
\newpage\tableofcontents

\section{¿Cuál es la complejidad de Edmonds-Karp?}
\label{sec:org6b211e4}
Para esta demostración usaré las definiciones y resultados de \hyperref[sec:org95ea031]{\uline{este teorema}}.
\textbf{La complejidad de EK es \(O(n \times m^2)\)}. EK encuentra un camino aumentante de s a t y aumenta el flujo a lo largo de este, esto lo hace hasta que no encuentra más caminos aumentantes. Por lo tanto, su complejidad viene dada por:
\[(Compl(\text{\small aumentar flujo}) + Compl(\text{\small encontrar camino aumentante})) \times \# \text{\small caminos aumentantes} \]
La complejidad de aumentar el flujo en un camino es O(n), pues el camino tiene a lo sumo n vértices y tengo que recorrerlo dos veces. La complejidad de encontrar un camino aumentante es O(m) porque uso BFS. Luego, como \({n \le m}\) (en componentes conexas al menos):
\[Compl(EK) = O(m) \times \# \text{caminos aumentantes} \]
Entonces me basta probar que hay \(O(m \times n)\) caminos aumentantes. Para ello veré cuántas veces puede un lado volverse crítico, teniendo en cuenta que un lado se vuelve crítico en el paso k cuando para pasar al paso k+1 este se satura o vacía completamente. Esto lo hago porque por cada camino aumentante que encuentre, cuando aumente el flujo, voy a saturar o llenar al menos un lado en ese camino. Entonces si puedo acotar la cantidad de veces que un lado se vuelve crítico, habré acotado la cantidad de caminos aumentantes.
Entonces veré los diferentes casos en donde se satura un lado.

\subsection*{El lado \(\overrightarrow{xz}\) se satura en el paso k}
\label{sec:org471195e}
Esto significa que para pasar de \(f_k\) a \(f_{k+1}\) usé un \(f_k\)-camino aumentante de la forma \(s \cdots x z \cdots t\). Como estoy corriendo EK, este camino es el de menor longitud entre s y t, lo que implica que también es el de menor longitud entre s y x y z, entonces
\[ d_k(z) = d_k(x) + 1 \]
Ahora voy a ver qué pasa cuando \(\overrightarrow{xz}\) vuelve a hacerse crítico en un paso j. Esto puede ocurrir por dos razones:
\begin{itemize}
\item Si se usa backward en j y se vacía.
\item Si en un paso i entre k y j se usa backward y se vacía un poco, para luego en j usarse forward y saturarse.
\end{itemize}
De todas formas, \(\exists i : k < i \le j\) tal que \(\overrightarrow{xz}\) se usa backward en el paso i. Esto significa que para pasar de \(f_i\) a \(f_{i+1}\) uso un \(f_i\)-camino aumentante de la forma \({s \cdots \overleftarrow{zx} \cdots t}\), y por la misma razón que antes tengo que
\[d_i(x) = d_i(z) + 1\]
Juntando todo llego a
\begin{align*}
d_j(t) & \ge d_i(t) \\
       & = d_i(x) + b_i(x) \\
       & = d_i(z) + 1 + b_i(x) \\
       & \ge d_k(z) + 1 + b_k(x) \\
       & = d_k(x) + 1 + 1 + b_k(x) \\
       & = d_k(t) + 2
\end{align*}

\subsection*{El lado \(\overrightarrow{xz}\) se vacía en el paso k}
\label{sec:org2985558}
Esto significa que para pasar de \(f_k\) a \(f_{k+1}\) usé un \(f_k\)-camino aumentante de la forma \(s \cdots \overleftarrow{zx} \cdots t\). Como estoy corriendo EK, este camino es el de menor longitud entre s y t, lo que implica que 
\[ d_k(x) = d_k(z) + 1 \]
Ahora voy a ver qué pasa cuando \(\overrightarrow{xz}\) vuelve a hacerse crítico en un paso j. Esto puede ocurrir por dos razones:
\begin{itemize}
\item Si se usa forward en j y se satura.
\item Si en un paso i entre k y j se usa forward y se llena un poco, para luego en j usarse backward y vaciarse.
\end{itemize}
De todas formas, \(\exists i : k < i \le j\) tal que \(\overrightarrow{xz}\) se usa forward en el paso i. Esto significa que para pasar de \(f_i\) a \(f_{i+1}\) uso un \(f_i\)-camino aumentante de la forma \({s \cdots x z \cdots t}\), y por la misma razón que antes tengo que
\[d_i(z) = d_i(x) + 1\]
Juntando todo llego a
\begin{align*}
d_j(t) & \ge d_i(t) \\
       & = d_i(z) + b_i(z) \\
       & = d_i(x) + 1 + b_i(z) \\
       & \ge d_k(x) + 1 + b_k(z) \\
       & = d_k(z) + 1 + 1 + b_k(z) \\
       & = d_k(t) + 2
\end{align*}
\subsection*{Conclusión}
\label{sec:org5bec2a2}
En ambos casos llego a que si un lado se vuelve crítico, para que pueda volver a hacerlo la distancia entre s y t debe aumentar en al menos 2. Y como esta distancia está siempre entre 1 y n-1, esto puede pasar a lo sumo n/2 = O(n) veces. Como es para cada lado, tengo que en toda la corrida de EK, un lado cualquiera se vuelve crítico \(O(n \times m)\) veces, lo que significa que tengo \(O(n \times m)\) caminos aumentantes.


\section{Probar que \({d_k(x) \le d_{k+1}(x)}\)}
\label{sec:org95ea031}
\subsection*{Definiciones}
\label{sec:org0b24201}
Primero, recuerdo las definiciones. Para un flujo f y vértices x,z:
\begin{align*}
d_f(x, z) =
  \begin{cases}
    0 & \text{si } x = z \\
    \infty & \text{si no existe f-camino aumentante entre x y z} \\
    & \text{longitud del menor f-camino aumentante entre x y z}
  \end{cases}
\end{align*}
A partir de esta puedo definir para \({f_0, f_1, f_2, \cdots}\) la sucesión de flujos producidos por Edmonds-Karp:
\begin{align*}
d_k(x) = d_{f_k}(s, x) \\
b_k(x) = d_{f_k}(x, t) 
\end{align*}
Y además \({d_k(t) = d_k(x) + b_k(x)}\), si esos números son finitos.

\subsection*{Primera parte}
\label{sec:org3a1ae91}
Sea el conjunto \[ A = \{x \in V : d_{k+1}(x) < d_k(x)\}, \] si pruebo que este conjunto es vacío ya está.
Sup. que A no es vacío, entonces tomo \({x \in A}\) tal que \({ d_{k+1}(x) = min\{d_{k+1}(y) : y \in A\}. }\)
Además tengo que
\[ d_{k+1}(x) < d_k(x) \le \infty \implies d_{k+1}(x) < \infty, \tag{0}\]
lo que significa que existe un \(f_{k+1}\)-camino aumentante entre s y x. Y además como
\[d_{k+1}(s) = d_k(s) = 0 \implies s \not\in A \implies x \ne s,\]
sé que x tiene un elemento anterior en ese camino, el cual denotaré
\[ p_{k+1} : s \cdots z x \]
Ahora, como estoy usando E-K, ese camino es el de menor longitud entre s y x, lo que significa que también es el de menor longitud entre s y cualquier vértice anterior a x, porque si no podría encontrar un camino aún más corto a x. Entonces
\[ d_{k+1}(x) = d_{k+1}(z) + 1 \tag{1}\]
Luego, como \({d_{k+1}(z) < d_{k+1}(x)}\) y x es el elemento de A con menor \(d_{k+1}\), sé que \({z \not\in A}\), lo que implica que
\[ d_k(z) \le d_{k+1}(z) \tag{2} \]
Juntando 0, 1 y 2 llego a \({d_k(z) < \infty}\), lo que significa que existe un \(f_k\)-camino aumentante de longitud mínima entre s y z de la forma \({p_k: s \cdots z}\).

\subsection*{Si \(\overrightarrow{zx}\) es un lado}
\label{sec:orga26a94b}
En principio podría agregar x al final de \(p_k\) para obtener un \(f_k\)-camino aumentante entre s y x. Pero si hago eso llego a
\[ d_k(x) \le d_k(z)+1 \le d_{k+1}(z) + 1 = d_{k+1}(x), \]
lo que es un absurdo porque x está en A. Este absurdo tiene que venir de que no puedo agregar x al camino, y esto tiene que ser porque \({f_k(\overrightarrow{zx}) = c(\overrightarrow{zx})}\). Pero en \(p_{k+1}\) está el lado \(\overrightarrow{zx}\), lo que significa que para pasar de \(f_k\) a \(f_{k+1}\) tuve que usarlo en modo backward. Es decir, existe un camino de longitud mínima (por usar E-K)
\[ p_k': s \cdots \overleftarrow{xz} \cdots t, \]
entonces \({d_k(z) = d_k(x) + 1}\). Luego,
\begin{align*}
d_k(z) & = d_k(x) + 1 \\
       & > d_{k+1}(x) + 1 \\
       & = d_{k+1}(z) + 2 \\
       & \ge d_k(z) + 2
\end{align*}
Absurdo pues 0 < 2.

\subsection*{Si \(\overrightarrow{xz}\) es un lado}
\label{sec:orgf399c9d}
En principio podría agregar x al final de \(p_k\) para obtener un \(f_k\)-camino aumentante entre s y x. Pero si hago eso llego a
\[ d_k(x) \le d_k(z)+1 \le d_{k+1}(z) + 1 = d_{k+1}(x), \]
lo que es un absurdo porque x está en A. Este absurdo tiene que venir de que no puedo agregar x al camino, y esto tiene que ser porque \({f_k(\overrightarrow{zx}) = 0}\). Pero en \(p_{k+1}\) está el lado \(\overleftarrow{zx}\), lo que significa que para pasar de \(f_k\) a \(f_{k+1}\) tuve que usarlo en modo forward. Es decir, existe un camino de longitud mínima (por usar E-K)
\[ p_k': s \cdots xz \cdots t, \]
entonces \({d_k(z) = d_k(x) + 1}\). Luego,
\begin{align*}
d_k(z) & = d_k(x) + 1 \\
       & > d_{k+1}(x) + 1 \\
       & = d_{k+1}(z) + 2 \\
       & \ge d_k(z) + 2
\end{align*}
Absurdo pues 0 < 2.

\subsection*{Conclusión}
\label{sec:org0486899}
En ambos casos llego a un absurdo, y este absurdo viene de suponer que \({A \ne \emptyset}\), entonces A es vacío y finalmente \[ d_k(x) \le d_{k+1}(x) \]


\section{¿Cuál es la complejidad de Dinic en ambas versiones? No hace falta probar que la distancia en NAs sucesivos aumenta}
\label{sec:orgda88eee}
Probaré que \(Compl(Dinic) = O(n^2 \times m)\).
\subsection*{Complejidad de NAs}
\label{sec:orgd69164b}
Como en cada NA la distancia entre s y t aumenta, tendré a lo sumo O(n) NAs. Además, como uso BFS, la complejidad de construir cada NA es O(m).
Entonces tengo que
\begin{align*}
Compl(Dinic) & = (Compl(\text{hallar flujo bloqueante}) + Compl(construir NA)) \times \# \text{cantidad de NAs} \\
& = (Compl(\text{hallar flujo bloqueante}) + O(m)) \times O(n)
\end{align*}
Me basta probar que \(Compl(\text{hallar flujo bloqueante}) = O(n \times m)\).

\subsection*{Versión original}
\label{sec:org3c5dab7}
Los NAs en esta versión tienen la propiedad de que todo vértice tiene algún lado no saturado que va al siguiente nivel. Esto significa que DFS encuentra un camino de s a t sin tener que hacer backtracking. Entonces la complejidad de encontrar un camino en el NA es O(n° niveles) = O(n). Además, aumentar el flujo en un camino es O(n), pues hay que recorrer el camino dos veces. Ahora, como cada camino aumentante satura (al menos) un lado, tendré O(m) caminos aumentantes.
Entonces por ahora tengo que la complejidad de encontrar los caminos y aumentar el flujo en ellos es \({O(n \times m)}\). Sin embargo, tengo que ver cuánto cuesta mantener la propiedad que mencioné al principio.

Para mantener esa propiedad, luego de cada camino ejecuto el proceso 'podar', que va revisando los vertices desde los niveles superiores para abajo:
\begin{itemize}
\item Si el vertice tiene lados que salen no hace nada
\item Si no tiene lados que salen, borra el vértice y sus lados
\end{itemize}

Revisar si un vértice tiene lados que salen es O(1), y como luego de cada camino hago esto para cada vértice, esta parte me cuesta \(O(n \times m)\).

Ahora, borrar un vértice y sus lados se hace a lo sumo una vez por vértice y su complejidad es O(d(x)). Entonces, la complejidad sobre todos los 'podar' de esta parte es \(\displaystyle\sum_{x \in V} d(x) = O(m)\).

Finalmente, la complejidad de hallar flujo bloqueante es
\[ O(n \times m) + O(n \times m) + O(m) = O(n \times m) \]

\subsection*{Versión occidental}
\label{sec:org3031154}
Daré el algoritmo que se usa para encontrar un flujo bloqueante g:
\begin{verbatim}
g := 0
while (1) {
    p := [s]
    x := s
    while (x != t) {
        if (VecinosAdelanteDe(x) != []) {      // Avanzar (A)
            y := tomar de VecinosAdelanteDe(x)
            agregar y al final de p
            x := y
        } else {
            if (x != s) {                      // Retroceder (R)
                z := vértice anterior a x en p
                borrar zx
                x := z
            } else {
                return g
            }
        }
    }
    aumentar g en p y borrar lados saturados   // Incrementar (I)
}
\end{verbatim}

Notar que una corrida de este algoritmo es de la forma \({A \cdots AIA \cdots AIA \cdots AR \cdots}\) es decir, una sucesión de palabras de la forma \({A \cdots AX}\) donde X es I o R (excepto la última palabra de todas, que terminará con A). Además, \({O(A) = O(R) = O(1)}\) y \({O(I) = O(n)}\), pues para aumentar el flujo y borrar los lados tengo que recorrer el camino p dos veces y este tiene a lo sumo longitud n.

Tengo que ver cuantas As puede haber en una palabra. Como cada A me lleva al siguiente nivel del NA, hay O(n) As en cada palabra. De esta forma, el coste de las palabras me quedó:
\begin{itemize}
\item \(Compl(A....AR) = O(n) + O(1) = O(n)\)
\item \(Compl(A....AI) = O(n) + O(n) = O(n)\)
\end{itemize}

Solo basta ver cuantas de estas palabras puede haber. Como cada R borra un lado y cada I borra al menos un lado, tengo que hay a lo sumo m palabras.

Finalmente:
\[ Compl(\text{hallar flujo bloqueante}) = O(n) \times O(m) = O(n \times m) \]


\section{¿Cuál es la complejidad del algoritmo de Wave? No hace falta probar que la distancia en NAs sucesivos aumenta}
\label{sec:org164d7e4}
Probaré que \(Compl(Wave) = O(n^3)\).

Al igual que en Dinic, como la distancia entre s y t aumenta en NAs sucesivos, tengo O(n) NAs, pues t no puede estar a una distancia mayor a n de s.
Ahora, como
\[Compl(Wave) = (Compl(\text{hallar flujo bloqueante}) + Compl(\text{construir NA})) \times O(n)\]
y \(Compl(\text{construir NA}) = O(m)\) pues uso BFS, bastaría probar que \[Compl(\text{hallar flujo bloqueante}) = O(n^2).\] Notar que \(O(n^2) + O(m) = O(n^2)\) pues \(m \le \binom n2 = O(n^2)\).

Voy a dividir el proceso de hallar un flujo bloqueante en casos.

Cuando estoy balanceando vértices hacia adelante:
\begin{itemize}
\item V: los pasos donde saturo un lado
\item P: los pasos donde no saturo un lado
\end{itemize}

Cuando estoy balanceando los vértices hacia atrás:
\begin{itemize}
\item S: los pasos donde vacío un lado
\item Q: los pasos donde no vacío un lado
\end{itemize}

\uline{V}:
Sup. que x le manda flujo a z y \(\overrightarrow{xz}\) se satura. Para que \(\overrightarrow{xz}\) vuelva a saturarse, primero tiene que vaciarse un poco, pero si se vacía es porque z se bloqueó y entonces le devolvió flujo a x. Pero como z está bloqueado (y nunca se desbloqueará), x no le va a volver a mandar flujo a z. Entonces, cada lado puede saturarse a lo sumo una vez. \(\therefore Compl(V) = O(m).\)

\uline{S}:
Sup. que x le devuelve flujo a z y \(\overrightarrow{zx}\) se vacía. Como x devolvió flujo significa que está bloqueado, entonces z no volverá a mandarle flujo, entonces \(\overrightarrow{zx}\) no se podrá llenar para volver a vaciarse. Esto implica que un lado se puede vaciar a lo sumo una vez. \(\therefore Compl(S) = O(m).\)

\uline{P}:
Cuando un vértice manda flujo a sus vecinos para balancearse, satura todos los lados, excepto quizás uno. Esto significa que para cada vértice en cada ola hacia adelante se satura parcialmente a lo sumo un lado. Entonces \(Compl(P) = O(n) \times \# \text{olas hacia adelante}.\)

Ahora, en cada ola hacia adelante (excepto la última) queda al menos un vértice desbalanceado, es decir que cada ola hacia adelante hace que se bloquee al menos un vértice. Y como no se desbloquean una vez bloqueados, tengo que \(\# \text{olas hacia adelante} = O(n). \therefore Compl(P) = O(n^2)\).

\uline{Q}:
Cuando un vértice devuelve flujo a los vecinos, vacía todos los lados excepto quizás uno. Esto significa que para cada vértice en cada ola hacia atrás se vacía parcialmente a lo sumo un lado. Entonces \(Compl(Q) = O(n) \times \# \text{olas hacia atrás} = O(n) \times \# \text{olas hacia adelante} = O(n^2)\).

\textbf{Finalmente,
\[Compl(\text{hallar flujo bloqueante}) = O(m) + O(n^2) + O(m) + O(n^2) = O(n^2).\]}


\section{Probar que el valor de todo flujo es menor o igual a la capacidad de todo corte. También que si f es flujo, es maximal si y solo si existe S corte con V(f) = Cap(S) y S es minimal. No hace falta probar que \(V(f) = f(S, \overline{S}) - f(\overline{S}, S)\)}
\label{sec:org057e01e}
\subsection*{Valor de todo flujo menor o igual a capacidad de todo corte}
\label{sec:org42800cb}
\[
f(\overline{S}, S) = \sum_{\substack{x \not\in S \\ y \in S \\ \overrightarrow{xy} \in E}} f(\overrightarrow{xy}) \ge 0 \text{ pues } f(\overrightarrow{xy}) \ge 0, \forall \overrightarrow{xy} \in E
\]
Entonces tengo \(V(f) = f(S, \overline{S}) - f(\overline{S}, S) \le f(S, \overline{S}) \le c(S, \overline{S}) = Cap(S)\)

\subsection*{\(\impliedby\) vuelta}
\label{sec:orge13ac7a}
Sea f flujo y S corte con \(V(f) = Cap(S)\). Para todo flujo g, por lo que probé arriba tengo \(V(g) \le Cap(S) = V(f) \implies\) f es maximal. Además, para todo corte T tengo \(Cap(T) \ge V(f) = Cap(S) \implies\) S es minimal.

\subsection*{\(\implies\) ida}
\label{sec:org94decf5}
Sea f flujo maximal. Voy a construir un S y probaré que es un corte minimal con V(f) = Cap(S).
\[ S = \{s\} \cup \{x \colon \text{existe f-camino aumentante de s a x}\} \]
\subsubsection*{S es corte}
\label{sec:org2ff6d51}
Sup. que no lo es. La única forma que pase esto es porque \(t \not\in S\), pues \(s \in S\). Pero esto significa que existe un f-camino aumentante de s a t, por lo que puedo aumentar f en ese camino. Absurdo porque f es maximal, luego S es corte.

\subsubsection*{V(f) = Cap(S)}
\label{sec:org3199c86}
Sean \(x \in S, y \not\in S, \overrightarrow{xy} \in E\).
\[ x \in S \implies \text{ existe f-camino aumentante de s a x} \]
\[ y \not\in S \implies \text{ no existe f-camino aumentante de s a y} \]
Entonces el camino \(s \cdots x y\) podría ser aumentante, pero no lo es, y esto solo puede ser si \(f(\overrightarrow{xy})=c(\overrightarrow{xy})\). Entonces
\[f(S, \overline{S}) = \sum_{\substack{x \in S \\ y \not\in S \\ \overrightarrow{xy} \in E}} f(\overrightarrow{xy}) = \sum_{\substack{x \in S \\ y \not\in S \\ \overrightarrow{xy} \in E}} c(\overrightarrow{xy}) = c(S, \overline{S}) = Cap(S) \]


Ahora sean \(x \not\in S, y \in S, \overrightarrow{xy} \in E\).
\[ x \not\in S \implies \text{ no existe f-camino aumentante de s a x} \]
\[ y \in S \implies \text{ existe f-camino aumentante de s a y} \]
Entonces el camino \(s \cdots \overleftarrow{yx}\) podría ser aumentante, pero no lo es, y esto solo puede ser si \(f(\overrightarrow{xy})=0\). Entonces
\[f(\overline{S}, S) = \sum_{\substack{x \not\in S \\ y \in S \\ \overrightarrow{xy} \in E}} f(\overrightarrow{xy}) = \sum_{\substack{x \not\in S \\ y \in S \\ \overrightarrow{xy} \in E}} 0 = 0\]

Finalmente: \(V(f) = f(S, \overline{S}) - f(\overline{S}, S) = Cap(S) - 0 = Cap(S)\)


\section{Probar que si G es un grafo conexo no regular entonces \(\chi(G) \le \Delta(G)\)}
\label{sec:orgc4c43d9}
Sea \(x\) tal que \(d(x) = \delta(G)\). Corro BFS empezando por \(x\) y guardo el orden inverso en que los vértices se visitaron. Ahora voy a correr greedy en este orden que acabo de guardar. Notar que en BFS todo vértice es incluido por un vértice que ya ha sido visitado, entonces en el orden inverso todo vértice tiene al menos un vecino por delante (excepto el x). Esto significa que para cada vértice \(y\), en el peor caso va a tener \(d(y)-1 < \Delta\) vecinos coloreados todos con un color distinto, entonces voy a poder elegir un color en \(\{1,\cdots, \Delta\}\). Finalmente, cuando llego a x, como \(d(x)=\delta<\Delta\), podré elegir algún color de \(\{1,\cdots, \Delta\}\) que no está usado por ningún vecino de x para colorearlo. De esta forma tengo un coloreo propio de G que usa (a lo sumo) \(\Delta\) colores, entonces \(\chi(G) \le \Delta(G)\).



\section{Probar que 2-COLOR es polinomial}
\label{sec:orgd77d4df}
Para ello voy a dar un algoritmo polinomial que colorea un grafo con dos colores. Luego probaré que si el coloreo que da mi algoritmo no es propio es porque en el grafo hay algún ciclo impar, lo que implicaría \(\chi(G) \ge 3\). Notar que voy a asumir que el grafo es conexo, sin embargo si no lo es, simplemente hay que correr el algoritmo en todas sus componentes conexas y ver que todas sean bipartitas.

\subsection*{Algoritmo}
\label{sec:orgf52796b}
Agarro un vértice x de G y corro BFS empezando en él. Luego a cada vértice y le asigno el color \(Nivel_{BFS}(y) \text{ mod 2}\). Esto es lo mismo que pintar a x del color 1, y luego cada vez que un vértice z agrega a un vértice y en BFS, asigno \(c(y) = 1 - c(z)\). Es claro que esta parte es O(m) por usar BFS.

Ahora tengo que revisar que el coloreo sea en efecto propio, lo que cuesta \({\displaystyle\sum_{x \in V} O(d(x)) = O(m)}\). Entonces llego a que mi algoritmo es polinomial.
\subsection*{¿Qué pasa cuando el coloreo que di no es propio?}
\label{sec:org6c81210}
Eso significa que \(\exists u,v \in V : c(u) = c(v) \land uv \in E\). Entonces
\begin{align*}
Nivel_{BFS}(u) \text{ mod 2} = Nivel_{BFS}(v) \text{ mod 2} \\
\implies Nivel_{BFS}(u) + Nivel_{BFS}(v) \text{ es par} \tag{1}
\end{align*}
Como u y v fueron agregados por un BFS desde x, sé que existe un camino de x a u y un camino de x a v. Ahora sea w el vértice tal que estos dos caminos se separan (notar que w puede ser x), voy a calcular la cantidad de lados en el ciclo \({w \cdots u v \cdots w}\).
\begin{itemize}
\item En \(w \cdots u\) hay \(Nivel_{BFS}(u) - Nivel_{BFS}(w)\) lados.
\item En \(uv\) hay 1 lado.
\item En \(v \cdots w\) hay \(Nivel_{BFS}(v) - Nivel_{BFS}(w)\) lados.
\end{itemize}
Luego en el ciclo hay
\begin{align*}
Nivel_{BFS}(u) - Nivel_{BFS}(w) + 1 + Nivel_{BFS}(v) - Nivel_{BFS}(w) \\
= Nivel_{BFS}(u) + Nivel_{BFS}(v) + 2 \times Nivel_{BFS}(w) + 1
\end{align*}
lados, que es un número impar por (1).


\section{Enunciar y probar el teorema de Hall}
\label{sec:org5ff0212}
El teorema de Hall dice que en un grafo bipartito con partes X e Y, existe un matching completo de X a Y si y solo si \({|\Gamma(S)| \ge |S|, \forall S \subseteq X}\). Notar que un matching M de X a Y es completo si |E(M)| = |X|.

\subsection*{\(\implies\) ida}
\label{sec:org836f2f9}
Sea M un matching completo de X a Y. Este me induce una función inyectiva \[f: X \rightarrow Y\] tal que \[f(x) \in \Gamma(x).\] Luego, como f es inyectiva, tengo que \[|f(S)| = |S|, \forall S \subseteq X.\]
Y además \[f(S) \subseteq \Gamma(S).\]
Finalmente, tengo que \(|S| \le |\Gamma(S)|, \forall S \subseteq X\).

\subsection*{\(\impliedby\) vuelta}
\label{sec:org6a86d52}
Sup. que se cumple \({|\Gamma(S)| \ge |S|, \forall S \subseteq X}\) (condición de Hall), pero que al correr el algoritmo de encontrar matching maximal llego a un M con \({|E(M)| < |X|.}\)
Trabajaré sobre la forma matricial del algoritmo.
Sean
\begin{align*}
S & = \{\text{\small filas etiquetadas}\} \\
T & = \{\text{\small columnas etiquetadas}\} \\
S_0 & = \{\text{\small filas etiquetadas con }\star\} \\
T_1 & = \{\text{\small columnas etiquetadas por }S_0\} \\
S_1 & = \{\text{\small filas etiquetadas por } T_1\}
\end{align*}
Y en general:
\begin{align*}
T_{i+1} & = \{\text{\small columnas etiquetadas por }S_i\} \\
S_i & = \{\text{\small filas etiquetadas por } T_i\}.
\end{align*}
Notar que como M no es completo, tengo algunas filas sin matchear, es decir que
\[S_0 \ne \emptyset \tag{0}\]
Además, es claro que
\begin{align*}
S & = S_0 \dot\cup S_1 \dot\cup S_2 \dot\cup \cdots \tag{1a} \\
T & = T_1 \dot\cup T_2 \dot\cup T_3 \dot\cup \cdots \tag{1b}
\end{align*}
Ahora, cuando estoy viendo una columna pueden pasar dos cosas:
\begin{itemize}
\item La columna está libre, entonces la matcheo y extiendo el matching. Esto no pasa porque M es maximal.
\item La columna está matcheada con una fila, entonces etiqueto \uline{únicamente} esa fila.
\end{itemize}
Así, es claro que
\[|T_i| = |S_i| \tag{2}\]
y que el algoritmo se detiene al pasar de un \(S_k\) a un \({T_{k+1} = \emptyset}\). Entonces llego a que
\begin{align*}
S & = S_0 \dot\cup \cdots \dot\cup S_k \\
T & = T_1 \dot\cup \cdots \dot\cup T_k 
\end{align*}
Juntando todo:
\begin{align*}
|S| & = |S_0| + |S_1| + \cdots + |S_k| \quad\text{por 1a} \\
    & = |S_0| + |T_1| + \cdots + |T_k| \quad\text{por 2} \\
    & = |S_0| + |T| \quad\text{por 1b} \\
    & > |T| \quad\text{por 0}
\end{align*}
Ahora voy a ver que \(T = \Gamma(S)\):
\begin{itemize}
\item \(T \subseteq \Gamma(S)\): sea \(y \in T\), \(y\) tuvo que ser etiquetado por una fila de S, y como cada fila etiqueta a sus columnas vecinas es claro que \({y \in \Gamma(S).}\)
\item \(\Gamma(S) \subseteq T\): sup. que existe un \(y \in \Gamma(S)\) que no está en T. Existe un \({x \in S}\) que es vecino de \(y\). Pero cuando revisé \(x\), habría visto que \(y\) era vecino suyo, y por lo tanto lo habría etiquetado. Absurdo pues supuse \({y \not\in T}\), luego, \({\Gamma(S) \subseteq T.}\)
\end{itemize}

Finalmente, construí un \(S \subseteq X\) tal que no se cumple la condición de Hall. Lo que es un absurdo pues yo supuse que era cierta. Entonces este absurdo viene de suponer que el matching maximal no es completo.


\section{Enunciar y probar el teorema del matrimonio de Koenig}
\label{sec:orga44925f}
Este teorema dice que todo grafo bipartito regular tiene un matchig perfecto. Un matching es perfecto si es completo en ambos sentidos.

Defino
\[ E_W = \{wu \in E : w \in W\} \quad\text{ para } W \subseteq V \]
Sean X e Y las partes del grafo. Sea \(S \subseteq X\) y \(l \in E_S.\) Entonces existen \({x \in S}\) y \({y \in Y}\) tales que \({l = xy}.\) Entonces \({y \in \Gamma(x) \subseteq \Gamma(S),}\) por lo que \({l \in E_{\Gamma(S)}.}\) Finalmente llego a
\[ E_S \subseteq E_{\Gamma(S)} \implies |E_S| \le |E_{\Gamma(S)}|, \forall S \subseteq X \]

Ahora calcularé cuanto vale \({|E_W|}\) para \({W \subseteq X}\) o \({W \subseteq Y.}\) Sea \({wu \in E_W}\), es claro que \({u \not\in W,}\) pues \({w \in X}\) o \({w \in Y},\) entonces
\[ E_W = \dot\bigcup_{w \in W} \{wu : u \in \Gamma(w)\} \]
Entonces
\begin{align*}
|E_W| & = \sum_{w \in W} |\{ wu : u \in \Gamma(w) \}| \\
      & = \sum_{w \in W} d(w) \\
      & = \sum_{w \in W} \Delta \quad\text{pues el grafo es regular} \\
      & = |W| \times \Delta
\end{align*}
Luego
\begin{align*}
|E_S| \le |E_\Gamma(S)| & \Leftrightarrow |S| \times \Delta \le |\Gamma(S)| \times \Delta \\
                        & \Leftrightarrow |S| \le |\Gamma(S)|
\end{align*}

Así, por teorema de Hall, sé que hay matching completo de X a Y. Pero la elección de X sobre Y en esta demostración fue arbitraria, por lo que la podría repetir para un \({S \subseteq Y}\) y llegar a que hay un matching completo de Y a X. Esto significa que \({|X| \le |Y|}\) y \({|X| \ge |Y|}\), por lo que \({|X| = |Y|,}\) entonces el matching es perfecto.


\section{Enunciar y probar el teorema de la cota de Hamming}
\label{sec:orgcf7d8a7}

\subsection*{Teorema}
\label{sec:org9d98267}
Para todo código \(C \in \{0, 1\}^n\) con \(t = \lfloor \frac{\delta-1}{2} \rfloor\):
                    \[ |C| \le \frac{2^n}{1 + n + \binom{n}{2} + \cdots + \binom{n}{t}} \]
\subsection*{Demostración}
\label{sec:org15ead2a}
Sea
          \[ A = \bigcup_{v \in C} D_t(v) \]
buscaré \(|A|\).

Como \(C\) corrige t errores, tengo que
      \[ D_t(v) \cap D_t(w) = \emptyset, \forall v,w \in C \text{ tales que } v \ne w \]
Luego, es claro que A es unión disjunta.

Ahora, defino
      \[ S_r(v) = \{ w \in C : d_H(v, w) = r \} \]
De esta forma es claro que
      \[ D_t(v) = \bigcup_{r = 0}^t S_r(v) \quad\text{unión disjunta}\]

Sea \(w \in S_r(v)\), hay un subconjunto de los n bits de las palabras que tiene r elementos tal que \(w\) difiere de \(v\) en esos r bits. Con esto en mente:
\begin{itemize}
\item Dado \(w \in S_r(v)\), puedo obtener r bits en los que \(v\) y \(w\) difieren.
\item Dado un conjunto de r bits, puedo obtener un \(w\) tal que \({ d_H(v, w) = r. }\)
\end{itemize}
Así, existe una biyección entre \(S_r(v)\) y el conjunto de subconjuntos de r bits. Entonces la cardinalidad de estos conjuntos es la misma. Finalmente:
      \[ |S_r(v)| = \binom{n}{r} \implies |D_t(v)| = \sum_{r=0}^t \binom{n}{r} \]

Juntando todo tengo:
\begin{align*}
  |A| & = \sum_{v \in C} |D_t(v)| \\
      & = \sum_{v \in C} \sum_{r=0}^t \binom{n}{r} \\
      & = |C| \times \sum_{r=0}^t \binom{n}{r} \\
      & \le 2^n \quad\text{pues } A \subseteq \{0, 1\}^n \\
      & \implies \\
|C| & \le \frac{2^n}{\displaystyle\sum_{r=0}^t \binom{n}{r}}
\end{align*}


\section{Probar que si \({C = Nu(H)}\), entonces \({\delta(C)}\) = tamaño del mínimo conjunto de columnas de H que es LD}
\label{sec:org325e0a4}
Sean \({m = \min\{j : \exists \text{ conjunto LD de j columnas de H}\}}\) y \({\delta = \delta(C).}\)

\subsection*{\(m \le \delta\)}
\label{sec:org249f9ec}
Como C es lineal,
      \[ \delta = \min\{|x| : x \in C \land x \ne 0  \} \]
Sea \(x\) tal que \({|x| = \delta.}\) \({\exists i_1, \cdots i_{\delta} }\) tales que \(x\) tiene un 1 en esas posiciones y un 0 en las demás. Entonces tengo que
      \[ x = e_{i_1} + \cdots + e_{i_\delta} \]
Además, como \({x \in C = Nu(H)}\), \({H \cdot x^T = 0.}\)

Luego,
\begin{align*}
	0 & = H \cdot x^T \\
    & = H \cdot (e_{i_1} + \cdots + e_{i_\delta})^T \\
    & = H \cdot (e_{i_1}^T + \cdots + e_{i_\delta}^T) \\
    & = H \cdot e_{i_1}^T + \cdots + H \cdot e_{i_\delta}^T) \\
    & = H^{(i_1)} + \cdots + H^{(i_\delta)}
\end{align*}

Entonces, como existe un subconjunto de \(\delta\) columnas que es LD, \({m \le \delta.}\)

\subsection*{\({\delta \le m}\)}
\label{sec:org7984161}
Por como definí \(m\), sé que hay un conjunto de \(m\) columnas que es LD. Sean \({H^{(j_1)}, \cdots, H^{(j_m)}}\) esas columnas. Luego, sea \({x = e_{j_1} + \cdots + e_{j_m}.}\)
\begin{align*}
	H \cdot x^T & = H \cdot (e_{j_1} + \cdots + e_{j_m})^T \\
              & = H \cdot (e_{j_1}^T + \cdots + e_{j_m}^T) \\
              & = H \cdot e_{j_1}^T + \cdots + H \cdot e_{j_m}^T \\
              & = H^{(j_1)} + \cdots + H^{(j_m)} \\
              & = 0
\end{align*}

Entonces \({ x \in Nu(H) = C, }\) y como \({x \ne 0,}\) tengo que \({\delta \le |x| = m. }\)


\section{Sea un código C de longitud n, dimensión k y polinomio generador g(x), probar que:}
\label{sec:org37237b8}
\begin{enumerate}
\item \({C = \{p(x) : gr(p) < n \land g(x) | p(x) \} = C_1}\)
\item \({C = \{v(x) \odot g(x) : v(x) \text{ es un polinomio cualquiera} \} = C_2}\)
\item \({ gr(g) = n - k }\)
\item \({ g(x) | (1+x^n) }\)
\end{enumerate}

\subsection*{1 y 2}
\label{sec:orgbfb8878}
Para probar esto, probaré que \({ C_1 \subseteq C_2 \subseteq C \subseteq C_1. }\)

\subsubsection*{\underline{\(C_1 \subseteq C_2\)}}
\label{sec:orgbc8aebd}
Sea \(p(x) \in C_1\), entonces existe \(q(x)\) tal que \({p(x) = g(x) \cdot q(x).}\) Además
      \[ n > gr(p) = gr(g(x) \cdot q(x)) \]
Entonces es claro que
      \[ p(x) = g(x) \cdot q(x) = g(x) \odot q(x) \in C_2 \]

\subsubsection*{\underline{\(C_2 \subseteq C\)}}
\label{sec:org3b1e98b}
Sea \({ p(x) = v(x) \odot g(x) \in C_2, }\) con \(v(x)\) un polinomio cualquiera de la forma
      \[ v(x) = v_0 + v_1 \cdot x + v_2 \cdot x^2 + \cdots + v_{gr(v)} \cdot x^{gr(v)} \]
Entonces:
\begin{align*}
p(x) & = v(x) \odot g(x) \\
     & = (v_0 + v_1 \cdot x + v_2 \cdot x^2 + \cdots + v_{gr(v)} \cdot x^{gr(v)}) \odot g(x) \\
     & = v_0 \odot g(x) + v_1 \cdot (x \odot g(x)) + v_2 \cdot (x^2 \odot g(x)) + \cdots + v_{gr(v)} \cdot (x^{gr(v)} \odot g(x)) \\
     & = v_0 \cdot g(x) + v_1 \cdot Rot(g(x)) + v_2 \cdot Rot^2(g(x)) + \cdots + v_{gr(v)} \cdot Rot^{gr(v)}(g(x)) \\
     & \in C
\end{align*}
Pues todas las rotaciones de \(g(x)\) están en \(C\).

\subsubsection*{\underline{\(C \subseteq C_1\)}}
\label{sec:org7720e0b}
Sea \(p(x) \in C,\) es claro que \({gr(p) < n,}\) falta ver que \({g(x) | p(x). }\) Entonces voy a dividir \(p\) por \(g\):
      \[ \exists q(x),r(x) : p(x) = g(x) \cdot q(x) + r(x) \land gr(r) < gr(g) \]

Ahora tomo módulo:
\begin{align*}
	p(x) & = p(x) \text{ mod } (1 + x^n) \\
	     & = (g(x) \cdot q(x) + r(x)) \text{ mod } (1 + x^n) \\
       & = g(x) \odot q(x) + (r(x) \text{ mod } (1 + x^n)) \\
       & = g(x) \odot q(x) + r(x) \quad\text{pues } gr(r) < gr(g) < n
\end{align*}
Entonces tengo que
      \[ r(x) = p(x) + g(x) \odot q(x) \]
Y como \(p \in C\) y \({g(x) \odot q(x) \in C_2 \subseteq C,}\) entonces \({ r \in C.}\) Pero como \(g\), es el generador, este es el único polinomio no nulo de menor grado en \({C,}\) y como \({gr(r)<gr(g),}\) entonces \({r(x) = 0.}\)

Finalmente, como \(g(x) | p(x)\), \({p(x) \in C_1.}\)

\subsection*{3}
\label{sec:orgadcb571}
Sea \(p(x) \in C\), entonces existe \(q(x)\) tal que \({ p(x) = g(x) \cdot q(x) .}\) Además \({n > gr(p) = gr(g) + gr(q) }\), entonces \({ gr(q) < n - gr(g) .}\)
Ahora, sea un \(q(x)\) tal que \(gr(q) < n - gr(g)\), tengo que \({ g(x) \cdot q(x) \in C. }\)

Es decir, hay una biyección entre \(C\) y el conjunto de polinomios de grado menor a \({n - gr(g).}\) Entonces:
\begin{align*}
	|C| & = |\text{conjunto de polinomios de grado menor a } n - gr(g)| \\
      \iff & \\
  2^k & = 2^{n - gr(g)} \\
      \iff & \\
    k & = n - gr(g) \\
      \iff & \\
    gr(g) & = n - k \\
\end{align*}

\subsection*{4}
\label{sec:org65ecb83}
Divido \(1+x^n\) por \(g(x)\):
      \[ \exists q(x), r(x) : 1 + x^n = g(x) \cdot q(x) + r(x) \land gr(r) < gr(g) \]
Ahora, si tomo módulo:
\begin{align*}
	0 & = (1 + x^n) \text{ mod } (1 + x^n) \\
    & = g(x) \cdot q(x) + r(x) \text{ mod } (1 + x^n) \\
    & = g(x) \odot q(x) + (r(x) \text{ mod } (1 + x^n)) \\
    & = g(x) \odot q(x) + r(x) \quad\text{pues } gr(r) < gr(g) < n \\
\implies \\
r(x) & = g(x) \odot q(x) \in C
\end{align*}
Pero como \(g\), es el polinomio de \(C\) no nulo de menor grado y \({gr(r) < gr(g),}\) entonces \({r(x) = 0}\) y \({g(x) | (1 + x^n).}\)


\section{Probar que 3SAT es NP-completo}
\label{sec:orgb846b61}
Como sé que SAT es NP-completo, probaré que SAT \(\le_P\) 3SAT. Primero tengo que dar un algoritmo polinomial que transforme instancias de SAT en instancias de 3SAT.

\subsection*{El algoritmo}
\label{sec:org0cf5802}
Sea \(B\) una expresión booleana con variables \({ x_1, \cdots, x_i }\) tal que
      \[ B = D_1 \land \cdots \land D_m \]
Donde cada \(D_j\) es una disjunción de literales \({l_{j1}, \cdots, l_{jr_j}.}\) Entonces voy a transformar a cada \(D_j\) en un \(E_j\) que será una conjunción de disjunciones de 3 literales:
\subsubsection*{\underline{Si \(r_j = 1\)}:}
\label{sec:org9a7699b}
Introduzco las variables \(y_{j1}, y_{j2}\):
    \[E_j = (l_{j1} \lor y_{j1} \lor y_{j2}) \land (l_{j1} \lor \overline{y_{j1}} \lor y_{j2}) \land (l_{j1} \lor y_{j1} \lor \overline{y_{j2}}) \land (l_{j1} \lor \overline{y_{j1}} \lor \overline{y_{j2}}) \]

\subsubsection*{\underline{Si \(r_j = 2\)}:}
\label{sec:orgffa3731}
Introduzco \(y_j\):
      \[E_j = (l_{j1} \lor l_{j2} \lor y_j) \land (l_{j1} \lor l_{j2} \lor \overline{y_j}) \]

\subsubsection*{\underline{Si \(r_j = 3\)}:}
\label{sec:org2d07266}
Entonces es claro que \(E_j = D_j\).

\subsubsection*{\underline{Si \(r_j \ge 4\)}:}
\label{sec:orgcb1da35}
Voy a introducir las variables \({ y_{j1}, \cdots, y_{j(r_j - 3)}\):
\begin{align*}
	E_j & = (l_{j1} \lor l_{j2} \lor y_{j1}) \\
      & \land (\overline{y_{j1}} \lor y_{j2} \lor l_{j3}) \\
      & \land (\overline{y_{j2}} \lor y_{j3} \lor l_{j4}) \\
      & \land \cdots \\
      & \land (\overline{y_{j(r_j-4)}} \lor y_{j(r_j-3)} \lor l_{j(r_j-2)}) \\
      & \land (\overline{y_{j(r_j-3)}} \lor l_{j(r_j-1)} \lor l_{jr_j}) 
\end{align*}

Es claro que este algoritmo es polinomial, porque si B tiene k literales, agrego a lo sumo k literales más (en realidad es menos que esto).

\subsection*{Los D son satisfacibles sii los E lo son}
\label{sec:org92dbb51}
Ahora que puedo transformar mi problema SAT en 3SAT, falta ver que mi B original es satisfacible si y solo si el nuevo lo es. Y como B es una conjunción de disjunciones, esto es lo mismo que ver que cada \(D_j\) es satisfacible \(\iff\) \(E_j\) lo es. Esto es lo mismo que ver que
      \[ D_j(\overrightarrow{b}) = 1 \iff \exists \overrightarrow{d} : E_j(\overrightarrow{b}, \overrightarrow{d}) = 1 \]
donde \(\overrightarrow{b}\) es el vector de asignación de las variables \(x\), mientras que \(\overrightarrow{d}\) es el de las \(y\).

Es trivial ver que esto se cumple para los \(D_j\) donde \(r_j \le 3\). Me quedan los otros casos.

\subsubsection*{\underline{\(\implies\) ida}}
\label{sec:org61874b0}
Sup. que \(D_j\) es satisfacible. Es decir, existe un vector de bits \(\overrightarrow{b}\) tal que \(D_j(\overrightarrow{b}) = 1\). Como \(D_j\) es una disjunción, hay un \(l_{jk}\) tal que \({l_{jk}(\overrightarrow{b}) = 1.}\) Tomo tal \(l_{jk}\) que cumpla eso (si hay más de uno, tomo el primero) y defino \(\overrightarrow{d}\):
      \[ d_1, \cdots, d_{k-2} = 1 \quad\text{y} \quad d_{k-1}, \cdots, d_{r_j - 3} = 0 \]

Ahora si evalúo \(E_j\), teniendo en cuenta que \({y_{ji}(\overrightarrow{d}) = d_i}\) y \({\overline{y_{ji}(\overrightarrow{d})} = 1 - d_i}\):
\begin{align*}
	E_j(\overrightarrow{b}, \overrightarrow{d}) & = (l_{j1} \lor l_{j2} \lor y_{j1})(\overrightarrow{b}, \overrightarrow{d}) \quad("=1" \text{ pues } d_1 = 1)\\
      & \land (\overline{y_{j1}} \lor y_{j2} \lor l_{j3})(\overrightarrow{b}, \overrightarrow{d}) \quad("=1" \text{ pues } d_2 = 1) \\
      & \land \cdots \\
      & \land (\overline{y_{j(k-3)}} \lor y_{j(k-2)} \lor l_{j(k-1)})(\overrightarrow{b}, \overrightarrow{d}) \quad("=1" \text{ pues } d_{k-2} = 1) \\
      & \land (\overline{y_{j(k-2)}} \lor y_{j(k-1)} \lor l_{jk})(\overrightarrow{b}, \overrightarrow{d}) \quad("=1" \text{ pues } l_{jk}(\overrightarrow{b}) = 1) \\
      & \land (\overline{y_{j(k-1)}} \lor y_{jk} \lor l_{j(k+1)})(\overrightarrow{b}, \overrightarrow{d}) \quad("=1" \text{ pues } d_{k-1} = 0) \\
      & \land \cdots \\
      & \land (\overline{y_{j(r_j-3)}} \lor l_{j(r_j-1)} \lor l_{jr_j})(\overrightarrow{b}, \overrightarrow{d}) \quad("=1" \text{ pues } d_{r_j-3} = 0) \\
      & = 1
\end{align*}


\subsubsection*{\underline{\(\impliedby\) vuelta}}
\label{sec:org1a98215}
Sup. que tengo  \(\overrightarrow{b}\) y \(\overrightarrow{d}\) tales que \(E_j(\overrightarrow{b}, \overrightarrow{d}) = 1\).  Ahora sup. que \(D_j(\overrightarrow{b}) = 0\). Esto implica que todas las \(l_{jq}\) se evalúan a 0 con \(\overrightarrow{b}\), y como \(E_j(\overrightarrow{b}, \overrightarrow{d}) = 1\) esto implica que 
\begin{align*}
	1 & = (y_{j1} \\
    & \land (\overline{y_{j1}} \lor y_{j2}) \\
    & \cdots \\
    & \land (\overline{y_{j(r_j-4)}} \lor y_{j(r_j-3)}) \\
    & \land \overline{y_{j(r_j-3)}}) (\overrightarrow{d})
\end{align*}
lo cual es claramente un absurdo. Luego, \(D_j(\overrightarrow{b}) = 1\).


\section{Probar que 3COLOR es NP-completo}
\label{sec:org7c51391}
Para ello probaré que 3SAT \(\le\) 3COLOR.

\subsection*{Algoritmo}
\label{sec:orgb6cc982}
Voy a crear un grafo G a partir de una expresión booleana \({B = D_1 \land \cdots \land D_m}\) con variables \(x_1, \cdots, x_i\), donde \(D_j = l_{j1} \lor l_{j2} \lor l_{j3}\).

\subsubsection*{Vértices}
\label{sec:orgb742329}
\begin{align*}
V(G) = & \{u_1, \cdots, u_i, w_1, \cdots, w_i\} \\
     \cup & \{a_{j1}, a_{j2}, a_{j3}\}_{j=1,\cdots,m} \\
     \cup & \{e_{j1}, e_{j2}, e_{j3}\}_{j=1,\cdots,m} \\
     \cup & \{CAPI, t\}
\end{align*}

\subsubsection*{Lados}
\label{sec:org6ecfddc}

\begin{itemize}
\item \underline{Triángulos}
\label{sec:org7d80ae7}
\[ \{u_it, tw_i, w_iu_i\}_{i=1,\cdots,n} \]

\item \underline{Garras}
\label{sec:orga9b14fa}
\[ \{a_{j1}a_{j2}, a_{j2}a_{j3}, a_{j3}a_{j1}\}_{j=1,\cdots,m} \cup \{a_{j1}e_{j1}, a_{j2}e_{j2}, a_{j3}e_{j3}\}_{j=1,\cdots,m} \]

\item \underline{Lados}
\label{sec:orgc585329}
\[\{(CAPI)e_{jr}\}_\substack{j=1,\cdots,m & r=1,2,3} \]

\item \underline{Lados}
\label{sec:org877aec4}
\[ \{(CAPI)t\}\]

\item \underline{Lados}
\label{sec:org74562f0}

Para estos voy a definir
\begin{align*}
v(l_{jr}) =
  \begin{cases}
    u_q & \text{si } \exists q : l_{jr} = x_q \\
    w_q & \text{si } \exists q : l_{jr} = \overline{x_q}
  \end{cases}
\end{align*}
Y luego los lados serán
\[ \{e_{jr}(v(l_{jr}))\}_\substack{j=1,\cdots,m & r=1,2,3} \]
\end{itemize}


\subsection*{B satisfacible sii \(\chi(G) \le 3\)}
\label{sec:org34ad62f}
\subsubsection*{\underline{\(\impliedby\) vuelta}}
\label{sec:orge4dae29}
Sup. \(\chi(G) \le 3\), como en G hay triángulos entonces \(\chi(G) = 3\). Sea C un coloreo propio de G que usa 3 colores, como \((CAPI)t\) es un lado, tengo que los colores posibles son \({\{C(CAPI), C(t), Z\}}\).
Ahora defino el vector de bits \(\overrightarrow{b}\):
\begin{align*}
b_i = 
\begin{cases}
  1 & \quad\text{si } C(u_i) = C(CAPI) \\
  0 & \quad\text{si } C(u_i) \ne C(CAPI)
\end{cases}	
\end{align*}

Para probar que \(B(\overrightarrow{b})\) tengo que ver que \({\forall j, \exists r : l_{jr}(\overrightarrow{b}) = 1.}\) Sea \({j \in \{1,\cdots,m\}}\). Como los \(a_{jr}\) forman un triángulo, hay un \(r\) tal que \(C(a_{jr}) = C(t)\).
\begin{align*}
	e_{jr}a_{jr} \in E(G) & \implies C(e_{jr}) \ne C(a_{jr}) = C(t) \\
	e_{jr}(CAPI) \in E(G) & \implies C(e_{jr}) \ne C(CAPI)
\end{align*}

Lo que implica que \(C(e_{jr}) = Z\). Ahora
\begin{align*}
	e_{jr}v(l_{jr}) \in E(G) & \implies C(v(l_{jr})) \ne C(e_{jr}) = Z \\
	tv(l_{jr}) \in E(G) & \implies C(v(l_{jr})) \ne C(t)
\end{align*}

Entonces tengo que \(C(v(l_{jr})) = C(CAPI)\).

\begin{itemize}
\item \uline{Si \(v(l_{jr})\) es una variable}
\end{itemize}

Existe k tal que \(l_{jr} = x_k\). Entonces \(v(l_{jr}) = u_k\), y por lo tanto \(C(u_k) = C(CAPI)\). Por cómo definí \(\overrightarrow{b}\), tengo que \(b_k = 1\), entonces
      \[ l_{jr}(\overrightarrow{b}) = x_k(\overrightarrow{b}) = b_k = 1 \]

\begin{itemize}
\item \uline{Si \(v(l_{jr})\) es una negación de variable}
\end{itemize}

Existe k tal que \(l_{jr} = \overline{x_k}\). Entonces \(v(l_{jr}) = w_k\), y por lo tanto \(C(w_k) = C(CAPI)\). Como \(u_kw_k\) es un lado, tengo que \(C(u_k) \ne C(CAPI)\), entonces \(b_k = 0\). Finalmente
      \[ l_{jr}(\overrightarrow{b}) = \overline{x_k(\overrightarrow{b})} = 1 - b_k = 1 \]

Finalmente, como para cualquier j existe un r tal que \(l_{jr}(\overrightarrow{b}) = 1\), tengo que \(B(\overrightarrow{b}) = 1\).

\subsubsection*{\underline{\(\implies\) ida}}
\label{sec:org2e15e41}
Sup. que \(B(\overrightarrow{b})=1\), voy a dar un coloreo C con 3 colores y probaré que es propio en G. Primero, defino que \({C(CAPI) = ROJO}\) y \({C(t) = VERDE}\), por lo que es claro que no hay problema con el lado \(t(CAPI)\). Para los \(u\) y los \(w\) defino:
\begin{align*}
	C(u_i) = \begin{cases}
  	ROJO & \quad\text{si } b_i = 1 \\
  	NEGRO & \quad\text{si } b_i = 0 \\
  \end{cases}
  \quad
	C(w_i) = \begin{cases}
  	NEGRO & \quad\text{si } b_i = 1 \\
  	ROJO & \quad\text{si } b_i = 0 \\
  \end{cases}
\end{align*}

Entonces los triángulos formados por \(u_i, w_i, t\) no tienen problema ya que están pintados de (NEGRO, ROJO, VERDE) en algún orden.

Ahora tengo que ver cómo pintar los \(a\). Como \(B(\overrightarrow{b}) = 1\), sé que
      \[ \forall j, \exists k_j : l_{jk_j}(\overrightarrow{b})=1 \]
Entonces tomo ese \(k_j\) para cada \(j\) (si hay más de uno, tomo el más chico). Luego, para cada \(j \in \{1, \cdots, m\}\) defino:
      \[ C(a_{jk_j}) = VERDE \quad\text{y pinto los otros dos uno de ROJO y otro de NEGRO}\]
De esta forma, los triángulos de \(a\) no tienen problema.

Ahora, para pintar los \(e\):
      \[ C(e_{jk_j}) = NEGRO \quad C(e_{jr}) = VERDE \text{ para } r \ne k_j \]

Falta ver que los lados con \(e\) no tengan problema.
\begin{itemize}
\item \(e_{jk_j}a_{jk_j}\)

No genera problema pues \({C(e_{jk_j}) = NEGRO \ne VERDE = C(a_{jk_j})}\)

\item \(e_{jr}a_{jr}\) con \(r \ne k_j\)

No hay problema pues \({C(e_{jr}) = VERDE}\) y \({C(a_{jr}) = ROJO \text{ o } NEGRO}\)

\item \(e_{jr}CAPI\)

No hay problema pues \(C(e_{jr}) = NEGRO \text{ o } VERDE\) y \({C(CAPI) = ROJO}\)

\item \(e_{jr}v(l_{jr})\) con \(r \ne k_j\)

No hay problema pues \({C(e_{jr}) = VERDE}\) y \({C(v(l_{jr})) = ROJO \text{ o } NEGRO}\) porque \({v(l_{jr}) = u_i \text{ o } w_i}\) para algún i

\item \(e_{jk_j}v(l_{jk_j})\)

\begin{itemize}
\item Si \(l_{jk_j}\) es una variable

Entonces existe q tal que \({l_{jk_j} = x_q}\). Entonces
      \[1 = l_{jk_j}(\overrightarrow{b}) = x_q(\overrightarrow{b}) = b_q \]
Por lo que \({C(v(l_{jk_j})) = C(u_q) = ROJO}\), entonces no hay problema.

\item Si \(l_{jk_j}\) es una negación de una variable

Entonces existe q tal que \({l_{jk_j} = \overline{x_q}}\). Entonces
      \[1 = l_{jk_j}(\overrightarrow{b}) = \overline{x_q}(\overrightarrow{b}) = 1 - b_q \implies b_q = 0 \]
Por lo que \({C(v(l_{jk_j})) = C(w_q) = ROJO}\), entonces no hay problema.
\end{itemize}
\end{itemize}


\section{Probar que matrimonio trisexual es NP-completo}
\label{sec:orgb15ca89}
Para ello voy a probar que 3SAT \(\le_P\) MATRIMONIO-trisexual, entonces voy a dar un algoritmo para pasar de una instancia a otra.

\subsection*{El algoritmo}
\label{sec:orgea97b71}
Sea B una instancia de 3SAT con variables \(x_1, \cdots, x_n\) tal que \({B = D_1 \land \cdots \land D_m}\) con \(D_j = l_{j1} \lor l_{j2} \lor l_{j3}\). Voy a construir un 3-hipergrafo \(H\) con partes \(X\), \(Y\) y \(Z\). Para simplificar la notación usaré:
\begin{align*}
i & \in \{1, \cdots, n\} \\
j & \in \{1, \cdots, m\} \\
r & \in \{1, 2, 3\} \\
k & \in \{1, \cdots, m(n-1)\} \\
\end{align*}

\subsubsection*{Vértices}
\label{sec:org5661198}
\begin{align*}
	X & = \{a_{ij}\}_{i,j} \cup \{s_j\}_j \cup \{h_k\}_k \\
	Y & = \{b_{ij}\}_{i,j} \cup \{t_j\}_j \cup \{g_k\}_k \\
	Z & = \{u_{ij}, w_{ij}\}_{i,j}
\end{align*}

Un requisito para que haya matching perfecto es que \(|X| = |Y| = |Z|\), lo cual se cumple pues
\begin{align*}
	|X| & = nm + m + m(n-1) = 2nm \\
	|Y| & = nm + m + m(n-1) = 2nm \\
	|Z| & = nm + nm = 2nm
\end{align*}

\subsubsection*{Lados}
\label{sec:orge13a5e6}
Primero, defino
\[
v_{jr} = \begin{cases}
          u_{ij} & \quad\text{si } \exists i : l_{jr} = x_i \\	
          w_{ij} & \quad\text{si } \exists i : l_{jr} = \overline{x_i}
\end{cases}
\]
Luego, los lados del hipergrafo H serán \(E_0 \cup E_1 \cup E_2 \cup E_3\):
\begin{align*}
	E_0 & = \{\{a_{ij}, b_{ij}, u_{ij}\}\}_{i,j} \\
	E_1 & = \{\{a_{i(j+1)}, b_{ij}, w_{ij}\}\}_{i,j} \quad(\text{notar que si } j = m \text{, entonces } j+1 = 1) \\
	E_2 & = \{\{h_k, g_k, u_{ij}\}\}_{i,j,k} \cup \{\{h_k, g_k, w_{ij}\}\}_{i,j,k}\\
  E_3 & = \{\{s_j, t_j, v_{jr}\}\}_{j,r} \\
\end{align*}

\subsection*{Hay matching perfecto en H sii B es satisfacible}
\label{sec:orged3e6cd}
\subsubsection*{\underline{\(\implies\) ida}}
\label{sec:org915bcf3}
Sup. que existe \(M\) matching perfecto en \(H\). Ahora, sea un i cualquiera, si existe un j tal que
      \[\exists L \in E(M) \cap E_0 : a_{ij} \in L, \]
entonces la única forma que pase esto es si \({ L = \{a_{ij}, b_{ij}, u_{ij}\}. }\) Luego, como \(M\) es matching y sus lados son disjuntos, tengo que \(b_{ij}\) no puede aparecer en otro lado de \(M\). Pero como \(M\) es completo, \(a_{i(j+1)}\) tiene que estar en algún lado, entonces llego a que \({ \{a_{i(j+1)}, b_{i(j+1)}, u_{i(j+1)} \} \in E(M) .}\) De la misma forma llego a que \({ \{a_{i(j+2)}, b_{i(j+2)}, u_{i(j+2)} \} \in E(M) ,}\) y así sucesivamente hasta llegar a
      \[ \{a_{ij}, b_{ij}, u_{ij}\} \in E(M), \forall j \]
lo cual llamaré 'CASO 0' para i.

Con un análisis análogo pero suponiendo que existe un j tal que
      \[\exists L \in E(M) \cap E_1 : a_{ij} \in L, \]
llego a 
      \[ \{a_{i(j+1)}, b_{ij}, w_{ij}\} \in E(M), \forall j \]
lo cual llamaré 'CASO 1' para i.

Notar que los vértices \(a_{ij}\) solo aparecen en \(E_0 \cup E_1\), lo que implica que si no se cumple 'CASO 1' para i, es porque se cumple 'CASO 0' para i.

Con esto en mente voy a definir \(\overrightarrow{b}\):
\[
b_i = \begin{cases}
	1 $ \quad\text{si 'CASO 1' para } i \\
	0 $ \quad\text{si 'CASO 0' para } i
\end{cases}
\]

Ahora, para probar \(B(\overrightarrow{b}) = 1\), tengo que probar que \(\forall j, \exists r : l_{jr}(\overrightarrow{b}) = 1\). Entonces sea un j fijo, como \(M\) es perfecto, los vértices \(t_j\) y \(s_j\) deben estar en algún lado de \(M\). Entonces existe r tal que \(\{s_j, t_j, v_{jr}\} \in E(M)\).

\begin{itemize}
\item \uline{Si \(l_{jr}\) es una variable \(x_i\)}:

Entonces \(v_{jr} = u_{ij}\) y \(\{s_j, t_j, u_{ij}\} \in E(M)\). Entonces \(\{a_{ij}, b_{ij}, u_{ij}\} \not\in E(M)\), por lo que estoy en 'CASO 1' para i. Finalmente:
      \[ l_{jr}(\overrightarrow{b}) = x_i(\overrightarrow{b}) = b_i = 1 \]

\item \uline{Si \(l_{jr}\) es una negación de variable \(\overline{x_i}\)}:

Entonces \(v_{jr} = w_{ij}\) y \(\{s_j, t_j, w_{ij}\} \in E(M)\). Entonces \(\{a_{i(j+1)}, b_{ij}, w_{ij}\} \not\in E(M)\), por lo que estoy en 'CASO 0' para i. Finalmente:
      \[ l_{jr}(\overrightarrow{b}) = \overline{x_i}(\overrightarrow{b}) = 1 - b_i = 1 \]
\end{itemize}

\subsubsection*{\underline{\(\impliedby\) vuelta}}
\label{sec:org3fab16d}
Sea \(\overrightarrow{b}\) tal que \(B(\overrightarrow{b}) = 1\). Voy a construir un matching completo \(M\) con lados \(F_0 \cup F_1 \cup F_2 \cup F_3\) tales que \(F_n \subseteq E_n\) para \(n = 0, 1, 2, 3\).

      \[ F_0 = \{ \{a_{ij}, b_{ij}, u_{ij}\} : j = 1, \cdots, m \land b_i = 0 \} \]
      \[ F_1 = \{ \{a_{i(j+1)}, b_{ij}, w_{ij}\} : j = 1, \cdots, m \land b_i = 1 \} \]
Es claro que estos lados son disjuntos.

Para definir \(F_3\) voy a usar el hecho de que \(B(\overrightarrow{b}) = 1\), lo que significa que \({ \forall j, \exists r_j : l_{jr_j}(\overrightarrow{b}) = 1 .}\) Si en algún \(j\) hay más de un \(r_j\) que cumpla eso, tomo uno. De esta forma:
      \[ F_3 = \{ \{ s_j, t_j, v_{jr_j} \} \}_j \]
Es claro que los lados de \(F_3\) son disjuntos entre sí, pues elegí un solo \(r_j\).

\begin{itemize}
\item \textbf{¿Qué pasa si hay un vértice que esté en un lado de \(F_0\) y en un lado de \(F_3\)?}

Esto tiene que ser porque \(v_{jr_j} = u_{ij}\) y \(\{a_{ij}, b_{ij}, u_{ij}\} \in F_0\). Estas dos cosas implican que \(l_{jr_j} = x_i\) y \(b_i = 0\). Luego
	  \[ 1 = l_{jr_j}(\overrightarrow{b}) = x_i(\overrightarrow{b}) = b_i = 0 \]
es un absurdo, entonces los lados de \(F_3\) son disjuntos con los de \(F_0\).

\item \textbf{¿Qué pasa si hay un vértice que esté en un lado de \(F_1\) y en un lado de \(F_3\)?}

Esto tiene que ser porque \(v_{jr_j} = w_{ij}\) y \(\{a_{i(j+1)}, b_{ij}, w_{ij}\} \in F_1\). Estas dos cosas implican que \(l_{jr_j} = \overline{x_i}\) y \(b_i = 1\). Luego
	  \[ 1 = l_{jr_j}(\overrightarrow{b}) = \overline{x_i}(\overrightarrow{b}) = 1 - b_i = 0 \]
es un absurdo, entonces los lados de \(F_3\) son disjuntos con los de \(F_1\).
\end{itemize}

Por ahora \(M\) es matching, pero falta ver que sea perfecto. Para esto defino
      \[ N = \{z \in Z : z \text{ no está cubierto por un lado de } F_0 \cup F_1 \cup F_3 \} \]
y calcularé \(|N|\). Primero, es claro que \(|F_3|=m\). Luego, sea \(p = |\{i : b_i = 0\}|\) y \(q = |\{i : b_i = 1\}|\), es claro que \(p+q = n\) y que \(|F_0| = mp\) y \(|F_1| = mq\). Con todo esto puedo calcular
      \[ |Z - N| = |F_0| + |F_1| + |F_3| = mp + mq + m = m(p+q+1) = m(n+1) \]
y finalmente
      \[ |N| = |Z| - |Z-N| = 2mn - m(n+1) = m(n-1) \]

Entonces, existe una biyección \(f: \{1, \cdots, m(n-1)\} \rightarrow N\), por lo que defino
      \[ F_2 = \{\{g_k, h_k, f(k)\} : k \in \{1, \cdots, m(n-1)\} \} \]
que tiene lados disjuntos porque f es inyectiva y además cubre todos los \(z \in Z\) que no tenía cubiertos porque f es suryectiva. Además, es claro que sus lados son disjuntos con los de \(F_0 \cup F_1 \cup F_3\) por cómo definí \(f\).

Finalmente, como cubrí todos los lados de \(Z\), \(M\) es matching y \(|X| = |Y| = |Z|\), tengo que \(M\) es matching perfecto en H.
\end{document}
